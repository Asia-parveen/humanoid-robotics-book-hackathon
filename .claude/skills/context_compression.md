# Context Compression Skill

## Purpose
Convert large documents and content blocks into optimally-sized chunks for AI processing while maintaining semantic integrity and maximizing token efficiency.

## When to Use
- Preparing documents for RAG systems
- Compressing long-form content for AI context windows
- Optimizing document chunks for embedding models
- Reducing token usage in API calls
- Preprocessing large texts for LLM consumption

## Inputs
- Raw document content (any format)
- Maximum chunk size (tokens or characters)
- Overlap requirements between chunks
- Semantic boundary preferences
- Priority preservation indicators
- Format requirements

## Outputs
- Optimized content chunks
- Boundary preservation markers
- Semantic integrity scores
- Token usage statistics
- Compression ratio metrics
- Quality assessment reports

## Constraints
- Preserve essential meaning in each chunk
- Maintain contextual coherence
- Respect semantic boundaries (paragraphs, sentences)
- Optimize for token efficiency
- Avoid splitting related concepts
- Minimize information loss
- Ensure chunks are self-contained where possible
- Maintain original document structure when relevant